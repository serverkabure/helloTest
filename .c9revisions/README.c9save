{"ts":1363754979774,"silentsave":true,"restoring":false,"patch":[[{"diffs":[[1,"Repo layout\n=========== \ndeployments/ - location for built wars (Details below)\nsrc/ - Maven src structure\npom.xml - Maven build file  \n.openshift/ - location for openshift specific files\n.openshift/config/ - location for configuration files such as standalone.xml (used to modify jboss config such as datasources) \n../data - For persistent data (also in env var OPENSHIFT_DATA_DIR)\n.openshift/action_hooks/pre_build - Script that gets run every git push before the build (on the CI system if available)\n.openshift/action_hooks/build - Script that gets run every git push as part of the build process (on the CI system if available)\n.openshift/action_hooks/deploy - Script that gets run every git push after build but before the app is restarted\n.openshift/action_hooks/post_deploy - Script that gets run every git push after the app is restarted\n\n\nDetails about layout and deployment options\n==================\nThere are two options for deploying content to the JBoss Application Server within OpenShift:\n\n1) (Preferred) You can upload your content in a Maven src structure as is this sample project and on \ngit push have the application built and deployed.  For this to work you'll need your pom.xml at the \nroot of your repository and a maven-war-plugin like in this sample to move the output from the build\nto the deployments directory.  By default the warName is ROOT within pom.xml.  This will cause the \nwebapp contents to be rendered at http://app_name-namespace.rhcloud.com/.  If you change the warName in \npom.xml to app_name, your base url would then become http://app_name-namespace.rhcloud.com/app_name.\n\nNote: If you are building locally you'll also want to add any output wars/ears under deployments \nfrom the build to your .gitignore file.\n\nor\n\n2) You can git push prebuilt wars (with the corresponding .dodeploy file for exploded wars) into deployments/.  To do this\nwith the default repo you'll want to first run 'git rm -r src/ pom.xml' from the root of your repo.\n\nBasic workflows for deploying prebuilt content (each operation will require associated git add/commit/push operations to take effect):\n\nA) Add new zipped content and deploy it:\n\n1. cp target/example.war deployments/\n\nB) Add new unzipped content and deploy it:\n\n1. cp -r target/example.war/ deployments/\n2. touch deployments/example.war.dodeploy\n\nC) Undeploy currently deployed content:\n\n1. git rm deployments/example.war.dodeploy deployments/example.war\n\nD) Replace currently deployed zipped content with a new version and deploy it:\n\n1. cp target/example.war deployments/\n\nE) Replace currently deployed unzipped content with a new version and deploy it:\n\n1. git rm -rf deployments/example.war/\n2. cp -r target/example.war/ deployments/\n3. touch deployments/example.war.dodeploy\n\nWARNING:  If you go with option 2) there are a couple issues to keep in mind with both prebuilt and exploded \nwars.  With exploded wars the main issue is with committing binaries (class and jar files) can make merge \nconflicts tedious. With prebuilt wars the main issue is each time you modify the war and git push, it \ntakes up the size of the war file away from your OpenShift file system quota.  One alternative to this \n(other then using Maven from option 1) is to use rsync to push your war into the deployments folder.  You \nwould have to do this after each git push followed by 'rhc app restart -a appname'.  Example:\n\nrsync -avz localdir/deployments/ app_uuid@appname-namespace.rhcloud.com:~/appname/repo/deployments/\n\nNote: You can get the information in the uri above from running 'rhc domain show'\n\nIf you have already committed large files to your git repo, you rewrite or reset the history of those files in git\nto an earlier point in time and then 'git push --force' to apply those changes on the remote OpenShift server.  A \ngit gc on the remote OpenShift repo can be forced with (Note: tidy also does other cleanup including clearing log\nfiles and tmp dirs):\n\nrhc app tidy -a appname\n\n\nWhether you choose option 1) or 2) the end result will be the application \ndeployed into the deployments directory. The deployments directory in the \nJBoss Application Server distribution is the location end users can place \ntheir deployment content (e.g. war, ear, jar, sar files) to have it \nautomatically deployed into the server runtime.\n\nThe filesystem deployment scanner in AS 7 and later works differently from \nprevious JBoss AS releases. The scanner will no longer attempt to directly \nmonitor the deployment content and decide if or when the end user wishes \nthe content to be deployed. Instead, the scanner relies on a system of marker \nfiles, with the user's addition or removal of a marker file serving as a sort\nof command telling the scanner to deploy, undeploy or redeploy content.\n\nThe marker files always have the same name as the deployment content to which\nthey relate, but with an additional file suffix appended. For example, the \nmarker file to indicate the example.war should be deployed is named \nexample.war.dodeploy. Different marker file suffixes have different meanings.\n\nThe relevant marker file types are:\n\n.dodeploy     -- Placed by the user to indicate that the given content should \n                 be deployed into the runtime (or redeployed if already \n                 deployed in the runtime.)\n\n.deploying    -- Placed by the deployment scanner service to indicate that it \n                 has noticed a .dodeploy file and is in the process of \n                 deploying the content. This marker file will be deleted when \n                 the deployment process completes.\n              \n.deployed     -- Placed by the deployment scanner service to indicate that the \n                 given content has been deployed into the runtime. If an end \n                 user deletes this file, the content will be undeployed.\n               \n.faileddeploy -- Placed by the deployment scanner service to indicate that the \n                 given content failed to deploy into the runtime. The content \n                 of the file will include some information about the cause of \n                 the failure.\n\n.undeploying  -- Placed by the deployment scanner service to indicate that it \n                 has noticed a .deployed file has been deleted and the \n                 content is being undeployed. This marker file will be deleted \n                 when the undeployment process completes.\n              \n.undeployed   -- Placed by the deployment scanner service to indicate that the \n                 given content has been undeployed from the runtime. If an end \n                 user deletes this file, it has no impact.\n\n\nEnvironment Variables\n=====================\n\nOpenShift provides several environment variables to reference for ease\nof use.  The following list are some common variables but far from exhaustive:\n\n    System.getenv(\"OPENSHIFT_GEAR_NAME\")  - Application name\n    System.getenv(\"OPENSHIFT_GEAR_DIR\")   - Application dir\n    System.getenv(\"OPENSHIFT_DATA_DIR\")  - For persistent storage (between pushes)\n    System.getenv(\"OPENSHIFT_TMP_DIR\")   - Temp storage (unmodified files deleted after 10 days)\n\nWhen embedding a database using 'rhc app cartridge add', you can reference environment\nvariables for username, host and password:\n\n    System.getenv(\"OPENSHIFT_DB_HOST\")      - DB host\n    System.getenv(\"OPENSHIFT_DB_PORT\")      - DB Port\n    System.getenv(\"OPENSHIFT_DB_USERNAME\")  - DB Username\n    System.getenv(\"OPENSHIFT_DB_PASSWORD\")  - DB Password\n\nTo get a full list of environment variables, simply add a line in your\n.openshift/action_hooks/build script that says \"export\" and push.\n"]],"start1":0,"start2":0,"length1":0,"length2":7629}]],"length":7629}
